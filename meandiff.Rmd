---
title: "the 'mean difference and difference of means' problem"
output:
  md_document:
    variant: markdown_github
---

## the 'mean difference and difference of means' problem

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path='output/figures/meandiff')

movePoints <- function(x0y0, xy, d){
  total.dist <- apply(cbind(x0y0, xy), 1,
                      function(x) stats::dist(rbind(x[1:2], x[3:4])))
  p <- d / total.dist
  p <- 1 - p
  x0y0[,1] <- xy[,1] + p*(x0y0[,1] - xy[,1])
  x0y0[,2] <- xy[,2] + p*(x0y0[,2] - xy[,2])
  return(x0y0)
}


```

One problem with the approach commonly used is that there is an underlying assumption that the mean of the differences between color points of two different groups reflects the difference between the means of those groups. That is, if I take all pairwise distances between points and average those, I am getting the same result, or at least an approximation, as if I took the mean of those groups and calculated the distance between them. We can demonstrate that this logic is flawed, because it overlooks a small, but crucial, thing.

First, let's demonstrate the case in which it _is_ true, to understand when it's _not_. We'll do it in one dimension for simplicity, where the distance between two numbers is just the difference between them.

Let's create a function that calculates the distance between all points in X and all points in Y (but not within X or within Y, just between groups), and then takes the average of those distances.

```{r}
meandiff <- function(z1,z2){
  
  res <- vector('numeric', 0)
  
  for(i in 1:length(z1)){
    tmp <- z1[i] - z2
    res <- c(res,tmp)
  }
  
  mean(res)
}
```

(not at all an efficient function but will get the job done)

Now let's create two random samples, and we'll even make them different sample sizes (to show this is not a paired design).

```{r}
set.seed(43805)
x <- rnorm(10, 50, 20)
y <- rnorm(13, 70, 20)
```

Now we can show that

```{r}
mean(x) - mean(y)
```

gives us the same result as

```{r}
meandiff(x,y)
```

But what happens when we look at distances? Let's test with Euclidean distances for this example.

```{r}
meaneucdist <- function(z1, z2){
    
  res <- vector('numeric', 0)
  
  for(i in 1:length(z1)){
    tmp <- sqrt((z1[i]-z2)^2)
    res <- c(res,tmp)
  }
  
  mean(res)
}

meaneucdist(x,y)
```

Why does this give a different result? Well, that becomes very clear when we look at the formula for the Euclidean distance:

sqrt((x-y)^2)

Which, in one dimension, simplifies to... (drumrolls!)

|x - y|

Boom. There's our problem. 

The Euclidean distance, and by consequence the JND distance, is **translation-invariant**. It ignores _position in space_. This is a desireable property when calculating distances sometimes, because it means 3 - 2 = 2 - 3 = -2 - (-3) = -3 - (-2). That is, regardless on where those two points are in space, _their difference will have the same magnitude_. Which is good - but **not when comparing populations of points!**, because in that case, **the position of points relative to one another matters (a lot!)** 

In fact, the mean of Euclidean distances - mean(sqrt((xi - xj)^2)) - is related to the mean squared difference, or mean squared error - mean(sqrt((mean(x) - xi)^2)) - which is a measure of spread, like the standard deviation. _It therefore will increase as a function of the mean variance of the groups, not the mean distance between the groups_.

<img src="http://i.giphy.com/i6TQUuiT5hjSU.gif" width="250">

Say in the problem of dichromatism, if color was one-dimensional, and you had:

males = c(2, -2)

females = c(2, -2)

You'd want those two groups to be the same (mean difference = 0). But a translation-invariant metric of distance (such as the Euclidean distance) would tell you they have a mean difference of `r meaneucdist(c(2,-2), c(2,-2))`.

To make it clearer how this applies to the comparison between groups in color space, let's show this in two dimensions:
```{r, echo=FALSE}
par(pty='s')
plot(c(-2,2), c(2,-2), pch=19, col='#4575b4', 
     ylim=c(-3,3), xlim=c(-3,3), cex=2,
     ylab='',xlab='', yaxt='n', xaxt='n')
points(c(2,-2),c(2,-2), pch=19, col='#d73027', cex=2)

arrows(1.8, 2, -1.8, 2, length=0.1, code=3, angle=20, lwd=2, col=grey(0,0.8))
arrows(2, 1.8, 2, -1.8, length=0.1, code=3, angle=20, lwd=2, col=grey(0,0.8))
arrows(1.8, -2, -1.8, -2, length=0.1, code=3, angle=20, lwd=2, col=grey(0,0.8))
arrows(-2, -1.8, -2, 1.8, length=0.1, code=3, angle=20, lwd=2, col=grey(0,0.8))
points(c(0,0),c(0,0), pch=c(3,4), col=c('#4575b4','#d73027'), cex=2, lwd=2)
```
We can see that taking the average of those distance arrows *ignoring their directions* would give a value greater than zero (which is the distance between their means, represented by crosses). Euclidean (and JND) distances ignore position information, which is critical when comparing groups of points.